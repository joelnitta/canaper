---
title: "How many randomizations?"
author: "Joel H. Nitta"
output: rmarkdown::html_vignette
date: "`r format(Sys.time(), '%d %B, %Y')`"
vignette: >
  %\VignetteIndexEntry{How many randomizations?}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
bibliography: nrand_references.yaml
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  retina = 3
)
```

A central part of CANAPE is comparing observed values with randomizations. It is up to the user to set the number of randomizations... but how does one know if the number of randomizations is sufficient?

(**This vignette assumes a basic understanding of CANAPE, community data matrices, and randomizations**. If you aren't familiar with any of these, you should probably see the [the CANAPE example vignette](https://joelnitta.github.io/canaper/articles/canape.html) first).

## A word about randomization algorithms

There is a rich literature on choice of randomization algorithms in ecology (see references below for a taste), so I won't go into this much, but choice of algorithm can have a large impact on results, so it's important to understand.

The function used by `canaper` for randomizations, `picante::randomizeMatrix()`, offers four different algorithms to choose from[^1]: `frequecy`, `richness`, `independentswap` [@Gotelli2000], and `trialswap` [@Miklos2004].

[^1]: An additional algorithm, `rand_structured` is currently [only offered in Biodiverse](http://biodiverse-analysis-software.blogspot.com/2020/11/randomisations-how-randstructured.html). [Benchmark tests indicate](http://biodiverse-analysis-software.blogspot.com/2020/12/biodiverse-now-includes-independent.html) that `rand_structured` is faster than the `picante` swapping algorithms.

`frequency` and `richness` are fairly simple: they randomize the community matrix by either shuffling abundances within species (`frequency`; preserves column totals) or shuffling abundances within samples (`richness`; preserves row totals). They don't require setting `n_iterations` since a single randomization is generated by just randomizing everything within these constraints once.

`independentswap` and `trialswap` are more sophisticated, in that they preserve the row **and** column totals while randomizing the data. Both operate by swapping occurrences between pairs of species and sites; one such swap is a single iteration (the [Biodiverse blog has a nice explanation of the algorithm](http://biodiverse-analysis-software.blogspot.com/2020/12/biodiverse-now-includes-independent.html)). So they both require a large number of swaps to achieve sufficient randomization. The two algorithms are quite similar, but `trialswap` modifies `independentswap` so that it converges to the uniform distribution [@Miklos2004].

In most cases, algorithms like `independentswap` and `trialswap` that randomize both columns and rows of the community matrix are preferable over simpler algorithms like `frequency` and `richness` [@Connor1979], so for this example we will use `trialswap`. 

## Replicates vs. iterations

Before proceeding, we need to clarify some terminology. `cpr_rand_test()` includes two arguments, `n_reps` and `n_iterations`. These sound similar but refer to two very different things. 

`n_reps` is the number of random communities to simulate, and is used by all four randomization algorithms. For example, if `n_reps` is 100, will we be comparing each observed value (e.g., phylogenetic diversity, `pd_obs`), with 100 random replicates of `pd_obs`. If `n_reps` is too low, we will lack sufficiently statistical power to detect patterns in the data.

`n_iterations` is only used by the `independentswap` and `trialswap` algorithms. As described above, these algorithms generate a random community by swapping existing cells in the community matrix. In other words, they "mix" the matrix to randomize it. If `n_iterations` is too low, the randomized matrix won't be sufficiently mixed, and will still resemble the original matrix.

If either `n_reps` or `n_iterations` are set too high, it will take overly long to finish the calculations. So our goal is to set them sufficiently high to achieve proper randomization, but not so high `cpr_rand_test()` never finishes.

## Effect of `n_iterations`

First, let's explore the effect of `n_iterations`. There are various ways to do this, but here I will use a [Mantel test](https://en.wikipedia.org/wiki/Mantel_test), which compares two matrices. Specifically, I will calculate the correlation between a distance matrix based on the original community data and a distance matrix based on the randomized data. I will use `picante::randomizeMatrix()` to generate the random communities, which `cpr_rand_test()` also uses internally[^2].

[^2]: The `iterations` argument of `picante::randomizeMatrix()` is equivalent to the `n_iterations` argument of `cpr_rand_test()`).

First, let's load the packages used in this vignette.

```{r setup, message = FALSE}
library(canaper) # This package
library(picante) # For generating random communities
library(vegan) # For Mantel test
library(tictoc) # For timing
library(tidyverse) # For tidy code
```

Next we will test the effects of `n_iterations`, using the [test dataset that comes with `canaper`](https://joelnitta.github.io/canaper/reference/biod_example.html) (and [Biodiverse](https://github.com/shawnlaffan/biodiverse/tree/master/data)):

```{r test-n-iter}
# Calculate a distance matrix from the original community data matrix
comm_dist <- vegdist(biod_example$comm)

# Set random number seed so the results are reproducible
set.seed(12345)
dist_test_res <- tibble(
  # Set up a vector from 10 to 10^6 for testing n_iterations
  n_iter = magrittr::raise_to_power(10, 1:6),
  # Make one random community for each value of `n_iter`
  rand_comm = map(
    n_iter, 
    ~randomizeMatrix(
      biod_example$comm, 
      null.model = "independentswap", iterations = .)),
  # Calculate the distance matrix of each community
  rand_comm_dist = map(rand_comm, vegdist),
  # Run the Mantel text and extract `r` (coefficient value) between
  # each random community and the original community
  mantel_r = map_dbl(rand_comm_dist, ~mantel(comm_dist, .)$statistic)
)

# Plot the results
ggplot(dist_test_res, aes(x = n_iter, y = mantel_r)) + 
  geom_point() +
  geom_line() +
  scale_x_log10() +
  labs(x = "n_iterations", y = "Mantel r")
```

From this, we can see that the original community and the randomized community are correlated up to about 1000 iterations. After that, the correlation coefficient levels off, and doesn't decrease much further with additional "mixing".

@Miklos2004 suggest for their algorithm (`trialswap`) to use at least twice the number of non-zeros in the community matrix. For our data, that is:

```{r miklos-rec}
comm_presabs <- biod_example$comm
comm_presabs[comm_presabs > 0] <- 1
sum(rowSums(comm_presabs)) * 2
```

So that roughly agrees with our plot above.

Note that the number of iterations required **will vary based on the dataset**. It will likely be higher in datasets that are extremely skewed towards a high percentage of presences or absences, since a given swap is unlikely to actually change anything. So I recommend exploring the data as above, or at least checking the "rule of thumb" of @Miklos2004, to determine the minimum number of iterations needed.

Another note: `picante::randomizeMatrix()` is quite fast, even with millions of swaps. So you can probably be comfortable running up to 10^6 or so, but again it will depend on the dataset.

Now that we've settled on the number of iterations per random replicate, let's look into the number of replicates. 

## Effect of `n_reps`

With randomizations, there is no "right" answer, so we can't test to see that `cpr_rand_test()` produces the exact answer we're looking for. Rather, we will check that it starts to **converge on approximately the same result** once `n_reps` is high enough. 

Here, I will compare the percentile of observed phylogenetic diversity relative to random (`pd_obs_p_upper`, [one of the values used for calculating endemism type](https://joelnitta.github.io/canaper/articles/canape.html#classify-endemism)) between pairs of random communities each generated with the same number of replicates[^3]. I will also time calculations for one of each pair.

[^3]: Other values based on the randomizations could be checked too (e.g., values ending in `_obs_z`, `_rand_mean`, or `_rand_sd`).

```{r n-reps-sets}
# Specify a different random seed for each set of randomizations so they give
# different, reproducible results

# First set
set.seed(12345)
tic()
res_10_1 <- cpr_rand_test(
  biod_example$comm, biod_example$phy, 
  n_iterations = 1000, n_reps = 10, tbl_out = TRUE)
toc()
tic()
res_100_1 <- cpr_rand_test(
  biod_example$comm, biod_example$phy, 
  n_iterations = 1000, n_reps = 100, tbl_out = TRUE)
toc()
tic()
res_1000_1 <- cpr_rand_test(
  biod_example$comm, biod_example$phy, 
  n_iterations = 1000, n_reps = 1000, tbl_out = TRUE)
toc()

# Second set
set.seed(67890)
res_10_2 <- cpr_rand_test(
  biod_example$comm, biod_example$phy, 
  n_iterations = 1000, n_reps = 10, tbl_out = TRUE)
res_100_2 <- cpr_rand_test(
  biod_example$comm, biod_example$phy, 
  n_iterations = 1000, n_reps = 100, tbl_out = TRUE)
res_1000_2 <- cpr_rand_test(
  biod_example$comm, biod_example$phy, 
  n_iterations = 1000, n_reps = 1000, tbl_out = TRUE)
```

Next, plot the results.

```{r n-reps-plots}
# We will make the same plot repeatedly, so write
# a quick function to avoid lots of copying and pasting
plot_comp <- function(res_1, res_2) {
  left_join(
  select(res_1, site, pd_obs_p_upper_1 = pd_obs_p_upper), 
  select(res_2, site, pd_obs_p_upper_2 = pd_obs_p_upper), by = "site") |>
  ggplot(aes(x = pd_obs_p_upper_1, y = pd_obs_p_upper_2)) + 
  geom_point() +
  geom_abline(slope = 1, intercept = 0)
}

plot_comp(res_10_1, res_10_2) + labs(title = "10 replicates")
plot_comp(res_100_1, res_100_2) + labs(title = "100 replicates")
plot_comp(res_1000_1, res_1000_2) + labs(title = "1000 replicates")
```

This visualization shows how the randomization results converge as `n_reps` increases.

The above plots illustrate convergence for one particular aspect of CANAPE, but what about endemism type itself? We can look at that too.

```{r comp-canape}
# Define a helper function that joins two datasets and calculates % agreement
# on endemism type between them
calc_agree_endem <- function(df_1, df_2, n_reps) {
  left_join(
    df_1 |> cpr_classify_endem() |> select(site, endem_type_1 = endem_type),
    df_2 |> cpr_classify_endem() |> select(site, endem_type_2 = endem_type),
    by = "site"
  ) |>
    mutate(agree = endem_type_1 == endem_type_2) |>
    summarize(agree = sum(agree), total = n()) |>
    mutate(
      p_agree = agree/total,
      n_reps = n_reps) 
}

bind_rows(
  calc_agree_endem(res_10_1, res_10_2, 10),
  calc_agree_endem(res_100_1, res_100_2, 100),
  calc_agree_endem(res_1000_1, res_1000_2, 1000),
)
```

At 1000 replicates, we see > 95% agreement on endemism type between the two randomizations. So here, I would use at least 100, and probably 1000 to be safe.

Of course, another important consideration is **how long** calculations take. You can see that time increases with `n_reps`, but not exactly in a linear fashion. We don't have the space to go into benchmarking here, but this illustrates the time / `n_reps` trade-off[^4].

[^4]: You can of course speed things up with parallelization. For details, see [the CANAPE example vignette](https://joelnitta.github.io/canaper/articles/canape.html#randomization-test).

## Conclusion

In this case (the [example dataset](https://joelnitta.github.io/canaper/reference/biod_example.html) that comes with `canaper`), we see that a minimum of 1000 random replicates with 1000 swapping iterations per replicate is probably needed to attain robust results. I hope this vignette helps you determine the settings to use for your own dataset!

## References
